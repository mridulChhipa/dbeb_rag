import os
os.environ["GOOGLE_API_KEY"] = "YOUR_API_KEY_HERE"

import torch
import logging
logging.getLogger('google.api_core').setLevel(logging.WARNING)
logging.getLogger('grpc').setLevel(logging.WARNING)

import asyncio
import uuid

from langgraph.prebuilt import ToolNode, tools_condition

from langgraph.graph import StateGraph, START, END, MessagesState
from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver

from langchain_community.storage import SQLStore

from langchain_huggingface import HuggingFaceEmbeddings

from langchain_qdrant import QdrantVectorStore

from qdrant_client import QdrantClient

from langchain_core.tools import Tool
from langchain_core.messages import HumanMessage, AnyMessage, AIMessageChunk
# from langchain_core.pydantic _v1import BaseModel, Field

from langchain_google_genai import ChatGoogleGenerativeAI

if torch.cuda.is_available():
    device = 'cuda'
    print("‚úÖ CUDA is available. Using GPU for embeddings.")
else:
    device = 'cpu'
    print("‚ö†Ô∏è CUDA not available. Using CPU for embeddings. This may be slow.")

model_kwargs = {
    'device': device
}

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2", 
    model_kwargs=model_kwargs
)

url = "http://localhost:6333"
client = QdrantClient(url=url, prefer_grpc=False)

qdrant = QdrantVectorStore(
    client=client,
    collection_name="dbeb", # The name of your existing collection
    embedding=embeddings,
)

retriever = qdrant.as_retriever()
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.5)

class State(MessagesState):
    pass

retriever_tool = Tool(
    name="document_retriever",
    description="Searches and returns relevant information and context from a knowledge base of documents.",
    func=retriever.invoke,
)

tools = [retriever_tool]
llm_with_tools = llm.bind_tools(tools)

async def agent_node(state: State):
    """
    The agent node that decides whether to call a tool or respond directly.
    """
    print("NODE: AGENT")
    messages = state["messages"]
    response = await llm_with_tools.ainvoke(messages)
    return {
        "messages": [response]
    }

tool_node = ToolNode(tools=tools)

async def run_chatbot(graph):
    thread_id = str(uuid.uuid4())
    config = {"configurable": {"thread_id": thread_id}}
    
    print("\n\nü§ñ Agentic RAG Chatbot is ready!")
    print(f"Session ID: {thread_id}")
    print("Ask your questions. Type 'exit' to quit.")
    
    while True:
        # Use asyncio.to_thread to run the blocking input() in an async-friendly way
        user_input = await asyncio.to_thread(input, "\nYou: ")
        if user_input.lower() in ["exit", "quit"]:
            print("Bot: Goodbye! üëã")
            break
            
        message = HumanMessage(content=user_input)
        
        try:
            print("Bot: ", end="", flush=True)
            
            # Use astream_events with version "v2" for the most detailed event stream.
            async for event in graph.astream_events({"messages": [message]}, config, version="v2"):
                kind = event["event"]
                
                # This is the canonical way to listen for token streams.
                if kind == "on_chat_model_stream":
                    chunk = event["data"]["chunk"]
                    if isinstance(chunk, AIMessageChunk) and not chunk.tool_call_chunks:
                        print(chunk.content, end="", flush=True)
            
            print() # Final newline
            
        except Exception as e:
            print(f"\n\n[ERROR] An error occurred: {e}")
            print("Please try your query again.")


# 5. Build and Compile the Graph, and run with asyncio
async def main():
    print("Building the agentic graph...")
    # AsyncSqliteSaver requires an async context manager
    async with AsyncSqliteSaver.from_conn_string("chatbot_memory.sqlite") as memory:
        workflow = StateGraph(State)

        workflow.add_node("agent", agent_node)
        workflow.add_node("tools", tool_node)

        workflow.set_entry_point("agent")

        workflow.add_conditional_edges(
            "agent",
            tools_condition,
        )
        
        workflow.add_edge("tools", "agent")

        graph = workflow.compile(checkpointer=memory)
        print("‚úÖ Agentic graph compiled successfully.")

        # Run the chatbot from within the async context
        await run_chatbot(graph)

if __name__ == "__main__":
    # Add nest_asyncio to patch the event loop for environments like Jupyter
    import nest_asyncio
    nest_asyncio.apply()

    # Run the main async function
    asyncio.run(main())
